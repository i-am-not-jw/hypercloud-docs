= EFK 스택 설치

EFK(Elastic search, FluentD, Kibana)는 FluentD 에이전트를 이용하여 각종 로그를 수집하며 Elastic search에 저장하고, 저장된 정보를 Kibana 대쉬보드로 시각화하여 보여주는 기능을 제공한다.

다음은 EFK 스택의 설치 과정에 대한 설명이다.

. *설치 파일 확인*
+
EFK 스택의 설치를 위해 필요한 tar 파일을 확인한다.
+
----
[root@master1 15.efk]# ls|grep tar
elasticsearch_7.2.1.tar    fluent.tar    kibana.tar
----

. *이미지 생성*
+
tar 파일을 `docker load` 명령을 사용해 도커 이미지로 생성한다. +
+
----
[root@master1 15.efk]# docker load -i elasticsearch_7.2.1.tar
[root@master1 15.efk]# docker load -i fluent.tar
[root@master1 15.efk]# docker load -i kibana.tar
[root@master1 15.efk]# docker load -i busybox.tar
----

. *이미지 업로드*
+
생성된 도커 이미지를 프라이빗 이미지 레지스트리에 업로드한다. 
+
우선 `docker tag` 명령을 사용해 각 이미지에 레지스트리의 주소를 추가한다.
+
----
[root@master1 15.efk]# docker tag \
docker.elastic.co/elasticsearch/elasticsearch:7.2.1 \
{IMAGE_REGISTRY}/docker.elastic.co/elasticsearch/elasticsearch:7.2.1
[root@master1 15.efk]# docker tag \
fluent/fluentd-kubernetes-daemonset:v1.4.2-debian-elasticsearch-1.1 \
{IMAGE_REGISTRY}/fluent/fluentd-kubernetes-daemonset:v1.4.2-debian-elasticsearch-1.1
[root@master1 15.efk]# docker tag \
docker.elastic.co/kibana/kibana:7.2.1 \
{IMAGE_REGISTRY}/docker.elastic.co/kibana/kibana:7.2.1
[root@master1 15.efk]# docker tag \
busybox:latest {IMAGE_REGISTRY}/busybox:latest
----
+
이후 `docker push` 명령을 사용해 각 이미지를 레지스트리에 업로드한다.
+
----
[root@master1 15.efk]# docker push \
{IMAGE_REGISTRY}/docker.elastic.co/elasticsearch/elasticsearch:7.2.1 
[root@master1 15.efk]# docker push \
{IMAGE_REGISTRY}/fluent/fluentd-kubernetes-\
daemonset:v1.4.2-debian-elasticsearch-1.1
[root@master1 15.efk]# docker push \
{IMAGE_REGISTRY}/docker.elastic.co/kibana/kibana:7.2.1
[root@master1 15.efk]# docker push {IMAGE_REGISTRY}/busybox:latest
----

. *업로드 확인*
+
이미지가 정상적으로 업로드되었는지 확인한다.
+
----
curl -X GET http://{IMAGE_REGISTRY}/v2/{이미지 이름}/tags/list
----

. *storageClass 설치*
+
00-storageClass.yaml 파일을 apply하여 storageClass를 설치한 후 확인한다.
+
----
[root@master1 15.efk]# kubectl apply -f 00-storageClass.yaml

[root@master1 15.efk]# kubectl get sc |grep local-storage
local-storage     kubernetes.io/no-provisioner    2d1h
----

. *영구 볼륨 설치*
+
01-pv-0.yaml, 02-pv-1.yaml, 03-pv-2.yaml 파일의 path와 node 정보를 수정한 후 apply하여 영구 볼륨을 생성한다. 
+
----
[root@master1 15.efk]# kubectl apply -f 01-pv-0.yaml
----
+
.path와 node 정보 수정 예
----
local:
    path: /root/data/volume1/pv0 <1>
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - master1 <2>
----
<1> 해당노드에 실제로 존재하는 path
<2> 영구 볼륨을 설치할 노드의 이름

. *영구 볼륨 생성 확인*
+
영구 볼륨이 정상적으로 생성됐는지 확인한다.
+
----
[root@master1 15.efk]# k get pv|grep local-pv
NAME        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS    REASON   AGE
local-pv    50Gi       RWO            Retain           Available           local-storage            28s
local-pv-1  50Gi       RWO            Retain           Available           local-storage            22s
local-pv-2  50Gi       RWO            Retain           Available           local-storage            17s
----

. *네임스페이스 생성*
+
04-namespaces.yaml 파일을 이용하여 네임스페이스를 추가한다.
+
----
[root@master1 15.efk]# kubectl apply -f 04-namespace.yaml
----

. *elasticsearch service 생성*
+
05-elasticsearch_svc.yaml 파일을 이용하여 elasticsearch Service를 추가한다.
+
----
[root@master1 15.efk]# kubectl apply -f 05-elasticsearch_svc.yaml
----

. *elasticsearch statefulset 설치*
+
06-elasticsearch_statefulset.yaml 파일의 {IMAGE_REPOSITORY} 부분에 구성한 docker registry IP를 넣어준 후 apply하여 설치한다.
+
----
[root@master1 15.efk]# kubectl apply -f 06-elasticsearch_statefulset.yaml
----

. *kibana 설치*
+
07-kibana.yaml 파일의 {IMAGE_REPOSITORY} 부분에 구성한 docker registry IP를 넣어준 후 apply하여 설치한다.
+
----
[root@master1 15.efk]# kubectl apply -f 07-kibana.yaml
----

. *fluentd 설치*
+
08-fluentd.yaml 파일에서 {IMAGE_REPOSITORY} 부분에 구성한 docker registry IP를 넣어준 후 apply하여 설치한다.
+
----
[root@master1 15.efk]# kubectl apply -f 08-fluentd.yaml
----
+
----
[root@master1 15.efk]# kubectl get svc -n kube-logging
NAME           TYPE          CLUSTER-IP    EXTERNAL-IP    PORT(S)             AGE
elasticsearch  ClusterIP     None          <none>         9200/TCP,9300/TCP   9m18s
kibana         LoadBalancer  10.96.77.15   172.22.9.138   5601:32083/TCP      5m35s
----
+
----
[root@master1 15.efk]# kubectl get pod -n kube-logging
NAME                      READY   STATUS    RESTARTS   AGE
es-cluster-0              1/1     Running   0          7m2s
es-cluster-1              1/1     Running   0          6m27s
es-cluster-2              1/1     Running   0          5m49s
fluentd-66cv2             1/1     Running   0          95s
fluentd-ndhjq             1/1     Running   0          95s
fluentd-rjwsp             1/1     Running   0          95s
kibana-57cb5b56c5-l77gr   1/1     Running   0          3m55s
----

. *접속 확인*
+
설정한 kibana Service의 IP 주소로 kibana 대시보드에 접속이 가능하다.